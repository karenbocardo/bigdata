Getting started. Why Hadoop? We have all heard that Hadoop and related projects in this
ecosystem are great for big data. This module will answer the four Ws and
an H about why this statement is true. Before we dive further into
the details of Hadoop, let's take a moment to analyze the
characteristics of the Hadoop ecosystem. What's in the ecosystem? Why is it beneficial? Where is it used? Who uses it? And how do these tools work? The Hadoop ecosystem frameworks and
applications that we will describe in this module have several overarching themes and
goals. First, they provide scalability
to store large volumes of data on commodity hardware. As the number of systems increases,
so does the chance for crashes and hardware failures. A second goal, supported by most
frameworks in the Hadoop ecosystem, is the ability to gracefully
recover from these problems. In addition, as we have mentioned before,
big data comes in a variety of flavors, such as text files,
graph of social networks, streaming sensor data and raster images. A third goal for
the Hadoop ecosystem then, is the ability to handle these different
data types for any given type of data. You can find several projects in
the ecosystem that support it. A fourth goal of the Hadoop ecosystem is the ability to facilitate
a shared environment. Since even modest-sized
clusters can have many cores, it is important to allow multiple
jobs to execute simultaneously. Why buy servers only to let them sit idle? Another goal of the Hadoop ecosystem
is providing value for your enterprise. The ecosystem includes a wide
range of open source projects backed by a large active community. These projects are free to use and
easy to find support for. In the following lectures in this module, we will take a more detailed
look at the Hadoop ecosystem. First, we will explore the kinds
of projects available and the types of capabilities they provide. Next we will take a deeper look at
the three main parts of Hadoop. The Hadoop distributed file system,
or HDFS. YARN, the scheduler and resource manager. And MapReduce, a programming model for
processing big data. We will then discuss cloud computing, and
the types of service models it provides. We will also describe situations in
which Hadoop is not the best solution. This module then concludes with
two readings involving hands-on experience with HDFS and MapReduce. So let's get started.